{"cells":[{"cell_type":"markdown","source":"# Data preparation\nThe purpose of this notebook is to prepare the data and gather information from it to be used for analysis.\n","metadata":{"tags":[],"cell_id":"00000-f3a52bf9-94f0-4fdf-a3e2-9f3857993046","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-c18a2c4e-d225-479e-829f-04fc676b67c4","deepnote_cell_type":"code"},"source":"import json\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport nltk\nfrom nltk import word_tokenize\nimport pandas as pd\nimport spotipy as sp\nimport os\nfrom spotipy.oauth2 import SpotifyClientCredentials","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment\nThe purpose of this section is to calculate the sentiment of every song and to assign an average sentiment to every artist.\nTo do this the dictionary `artist_song_id`, which was constructed during the construction and filtering of the network(`network_artists.ipynb`), is loaded.\nThe dictionary contains the song id of the remaining songs for every artist id in the network. ","metadata":{"tags":[],"cell_id":"00000-a9ae91a9-5a64-4e22-9e1f-a6bdfed76ddb","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-a0d208f0-5246-4399-8404-fa6de2ba1e78","output_cleared":false,"source_hash":"6525eb87","execution_start":1607173701668,"execution_millis":527,"deepnote_cell_type":"code"},"source":"## Load graph\nG = nx.read_graphml(\"../../data/graphs/G_final.graphml\")\n## Load dictionary file:\nwith open(\"../../data/other_files/artist_song_id.json\") as json_file:\n    artist_song_id = json.load(json_file)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To calculate the sentiment the wordlist from LabMT is downloaded and loaded:","metadata":{"tags":[],"cell_id":"00003-f374caac-7357-4ac3-bbc5-40d2cbf34610","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-be0f8a18-5ed9-4a8b-8267-01c83b125869","output_cleared":false,"source_hash":"fe6818d2","execution_millis":54,"execution_start":1607106192130,"deepnote_cell_type":"code"},"source":"df = pd.read_csv('../../data/sentiment/Data_Set_S1.txt', sep=\"\\t\", header=None)\ndf.columns = [\"word\", \"happiness_rank\", \"happiness_average\", \"happiness_standard_deviation\", \"twitter_rank\", \n                \"google_rank\", \"nyt_rank\", \"lyrics_rank\"]\ndf.head()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":8,"columns":[{"name":"word","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"laughter","count":1},{"name":"happiness","count":1},{"name":"3 others","count":3}]}},{"name":"happiness_rank","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":1,"max":5,"histogram":[{"bin_start":1,"bin_end":1.4,"count":1},{"bin_start":1.4,"bin_end":1.8,"count":0},{"bin_start":1.8,"bin_end":2.2,"count":1},{"bin_start":2.2,"bin_end":2.6,"count":0},{"bin_start":2.6,"bin_end":3,"count":0},{"bin_start":3,"bin_end":3.4000000000000004,"count":1},{"bin_start":3.4000000000000004,"bin_end":3.8000000000000003,"count":0},{"bin_start":3.8000000000000003,"bin_end":4.2,"count":1},{"bin_start":4.2,"bin_end":4.6,"count":0},{"bin_start":4.6,"bin_end":5,"count":1}]}},{"name":"happiness_average","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":8.26,"max":8.5,"histogram":[{"bin_start":8.26,"bin_end":8.283999999999999,"count":1},{"bin_start":8.283999999999999,"bin_end":8.308,"count":1},{"bin_start":8.308,"bin_end":8.332,"count":0},{"bin_start":8.332,"bin_end":8.356,"count":0},{"bin_start":8.356,"bin_end":8.379999999999999,"count":0},{"bin_start":8.379999999999999,"bin_end":8.404,"count":0},{"bin_start":8.404,"bin_end":8.428,"count":1},{"bin_start":8.428,"bin_end":8.452,"count":1},{"bin_start":8.452,"bin_end":8.475999999999999,"count":0},{"bin_start":8.475999999999999,"bin_end":8.5,"count":1}]}},{"name":"happiness_standard_deviation","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":0.9313,"max":1.1572,"histogram":[{"bin_start":0.9313,"bin_end":0.95389,"count":1},{"bin_start":0.95389,"bin_end":0.97648,"count":1},{"bin_start":0.97648,"bin_end":0.99907,"count":1},{"bin_start":0.99907,"bin_end":1.02166,"count":0},{"bin_start":1.02166,"bin_end":1.04425,"count":0},{"bin_start":1.04425,"bin_end":1.06684,"count":0},{"bin_start":1.06684,"bin_end":1.0894300000000001,"count":0},{"bin_start":1.0894300000000001,"bin_end":1.11202,"count":1},{"bin_start":1.11202,"bin_end":1.13461,"count":0},{"bin_start":1.13461,"bin_end":1.1572,"count":1}]}},{"name":"twitter_rank","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"3600","count":1},{"name":"1853","count":1},{"name":"3 others","count":3}]}},{"name":"google_rank","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"--","count":1},{"name":"2458","count":1},{"name":"3 others","count":3}]}},{"name":"nyt_rank","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"--","count":3},{"name":"328","count":1},{"name":"1313","count":1}]}},{"name":"lyrics_rank","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"1728","count":1},{"name":"1230","count":1},{"name":"3 others","count":3}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"word":"laughter","happiness_rank":1,"happiness_average":8.5,"happiness_standard_deviation":0.9313,"twitter_rank":"3600","google_rank":"--","nyt_rank":"--","lyrics_rank":"1728","_deepnote_index_column":0},{"word":"happiness","happiness_rank":2,"happiness_average":8.44,"happiness_standard_deviation":0.9723,"twitter_rank":"1853","google_rank":"2458","nyt_rank":"--","lyrics_rank":"1230","_deepnote_index_column":1},{"word":"love","happiness_rank":3,"happiness_average":8.42,"happiness_standard_deviation":1.1082,"twitter_rank":"25","google_rank":"317","nyt_rank":"328","lyrics_rank":"23","_deepnote_index_column":2},{"word":"happy","happiness_rank":4,"happiness_average":8.3,"happiness_standard_deviation":0.9949,"twitter_rank":"65","google_rank":"1372","nyt_rank":"1313","lyrics_rank":"375","_deepnote_index_column":3},{"word":"laughed","happiness_rank":5,"happiness_average":8.26,"happiness_standard_deviation":1.1572,"twitter_rank":"3334","google_rank":"3542","nyt_rank":"--","lyrics_rank":"2332","_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"        word  happiness_rank  happiness_average  happiness_standard_deviation  \\\n0   laughter               1               8.50                        0.9313   \n1  happiness               2               8.44                        0.9723   \n2       love               3               8.42                        1.1082   \n3      happy               4               8.30                        0.9949   \n4    laughed               5               8.26                        1.1572   \n\n  twitter_rank google_rank nyt_rank lyrics_rank  \n0         3600          --       --        1728  \n1         1853        2458       --        1230  \n2           25         317      328          23  \n3           65        1372     1313         375  \n4         3334        3542       --        2332  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>happiness_rank</th>\n      <th>happiness_average</th>\n      <th>happiness_standard_deviation</th>\n      <th>twitter_rank</th>\n      <th>google_rank</th>\n      <th>nyt_rank</th>\n      <th>lyrics_rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>laughter</td>\n      <td>1</td>\n      <td>8.50</td>\n      <td>0.9313</td>\n      <td>3600</td>\n      <td>--</td>\n      <td>--</td>\n      <td>1728</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>happiness</td>\n      <td>2</td>\n      <td>8.44</td>\n      <td>0.9723</td>\n      <td>1853</td>\n      <td>2458</td>\n      <td>--</td>\n      <td>1230</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>love</td>\n      <td>3</td>\n      <td>8.42</td>\n      <td>1.1082</td>\n      <td>25</td>\n      <td>317</td>\n      <td>328</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>happy</td>\n      <td>4</td>\n      <td>8.30</td>\n      <td>0.9949</td>\n      <td>65</td>\n      <td>1372</td>\n      <td>1313</td>\n      <td>375</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>laughed</td>\n      <td>5</td>\n      <td>8.26</td>\n      <td>1.1572</td>\n      <td>3334</td>\n      <td>3542</td>\n      <td>--</td>\n      <td>2332</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"The song lyrics however still needs a bit of preprocessing. \nEach song lyric is therefor tokenized, lowercased, cleaned of punctuation and other signs and lemmatized.","metadata":{"tags":[],"cell_id":"00006-d5ba0ed4-75d0-4042-ad11-8daa5a452d68","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-1c696c0f-5f9d-4d58-8c39-fbfed592f3ee","output_cleared":false,"source_hash":"4e350f9e","execution_millis":133,"execution_start":1607106194489,"deepnote_cell_type":"code"},"source":"# These downloads are needed to run the next code block\nnltk.download('punkt')\nnltk.download('wordnet')","execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-c2ffd679-6f99-46ff-87e6-1160c03cd031","output_cleared":false,"source_hash":"990f6629","execution_millis":1,"execution_start":1607106196479,"deepnote_cell_type":"code"},"source":"def text_preprocess(text_file):\n    # Tokenize\n    text1 = word_tokenize(text_file.lower())\n\n    # Remove punctuations and other signs\n    text2 = [word for word in text1 if word.isalnum()]\n\n    # Lemmatize\n    wnl = nltk.WordNetLemmatizer()\n    text3 = [wnl.lemmatize(t) for t in text2]\n\n    return text3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now calculate and store the sentiment for every song.","metadata":{"tags":[],"cell_id":"00003-3bdb6f09-1ca6-4d12-abab-c03912113d36","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-67a698b9-ffbc-4557-aa19-a29c90c9507f","output_cleared":false,"source_hash":"ebde1a11","execution_millis":182724,"execution_start":1607106327593,"deepnote_cell_type":"code"},"source":"song_sentiment = {}\nfor artist_id in G.nodes():\n    songs = artist_song_id[artist_id]\n    for song in songs:\n        text_file = open(\"../../data/genius/all_song_lyrics_cleaned/\" + song + \".txt\", \"r\").read()\n        temp_word = text_preprocess(text_file) # Preprocess the text file\n        temp_df = pd.DataFrame(data={'word':temp_word}).merge(df,on='word')\n        song_sentiment[song] = np.mean(temp_df.happiness_average)\n\n## Save the dictionary\nwith open(\"../../data/other_files/song_sentiment.json\", \"w\") as json_file:\n    json.dump(song_sentiment,json_file)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally the average sentiment for every artist in the network is calculated and added as an attribute to the network and the network is saved as ``G_final_sentiment.graphml``.","metadata":{"tags":[],"cell_id":"00011-c32e5d1d-fdbe-4723-8ba4-bcebd2add16e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-0d6f3d52-f33b-4ad9-963d-2754f8202ac1","output_cleared":false,"source_hash":"c45f3894","execution_millis":387,"execution_start":1607106795062,"deepnote_cell_type":"code"},"source":"for artist_id in G.nodes():\n    songs = artist_song_id[artist_id]\n    sentiment_temp = [] # to temporarily store the sentiment of each song\n    for song in songs:\n        sentiment_temp.append(song_sentiment[song])\n    G._node[artist_id]['sentiment'] = np.mean(sentiment_temp)\n\n# Save the graph\nnx.write_graphml(G,\"../../data/graphs/G_final_sentiment.graphml\",encoding='utf-8')","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataframes\nTo make analysis easier two seperate dataframes containing all information about the songs and the artists are constructed.\nThe year of the artist and song is further addede as this is also available from the Spotify API.\nA dataframe for the songs is first constructed.\n\n**Dataframe for songs**\n","metadata":{"tags":[],"cell_id":"00012-45486832-fb5d-439a-96d4-d47db117ad5f","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00013-2787afa1-8044-4b67-8961-6e086ec9fd49","output_cleared":false,"source_hash":"a22fffe0","execution_millis":2,"execution_start":1607171903930,"deepnote_cell_type":"code"},"source":"# Set the Spotify developer id and secret\nsp = sp.Spotify(auth_manager=SpotifyClientCredentials(\n    client_id=os.environ[\"SP_CLIENT_ANDREAS\"],\n    client_secret=os.environ[\"SP_SECRET_ANDREAS\"]\n    )\n)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00013-a3419b50-faa8-4981-a844-acb3cf7f72b4","output_cleared":false,"source_hash":"5ad2ff9b","execution_millis":1355455,"execution_start":1607171908245,"deepnote_cell_type":"code"},"source":"## Retrieve the date of the song from Spotify\nsong_year = {}\nfailed_songs = []\n\nfor song_id in song_sentiment.keys():\n    try:\n        track = sp.track(song_id) # Call spotify API\n        song_year[song_id] = track['album']['release_date'] # Get the date\n        time.sleep(0.05)\n    except:\n        failed_songs.append(song_id)\n\nprint(\"Number of failed songs:\", len(failed_songs))\n\n# Save the file\nwith open(\"../../data/other_files/song_year.json\", \"w\") as json_file:\n    json.dump(song_year,json_file)","execution_count":null,"outputs":[{"name":"stdout","text":"Number of failed songs: 13691\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-84e34602-b4c7-4fea-ae89-9f4fdc3c8184","output_cleared":false,"source_hash":"c9d159d6","execution_millis":0,"execution_start":1607173707362,"deepnote_cell_type":"code"},"source":"## Dictionary containing the artist id for every song id\nsong_artist = {}\nfor artist_id in G.nodes():\n    songs = artist_song_id[artist_id]\n    for song in songs:\n        song_artist[song] = artist_id","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-557886af-b497-4bba-9c55-fd121501150c","output_cleared":false,"source_hash":"1a6ff70f","execution_start":1607173717192,"execution_millis":59548,"deepnote_cell_type":"code"},"source":"# construct dataframe for song information\ndf_song_info = pd.DataFrame(columns=['song_id','artist','artist_id','genre','sentiment','year'])\n\nfor song_id in song_year.keys():\n    # Get info for new row of dataframe\n    year = song_year[song_id][0:4]\n    artist_id = song_artist[song_id]\n    artist = G._node[artist_id]['name']\n    genre = G._node[artist_id]['genre']\n    sentiment = song_sentiment[song_id]\n    # Specify and append new row\n    new_row = {'song_id':song_id,'artist': artist,'artist_id':artist_id,\n               'genre':genre,'sentiment':sentiment,'year':year}\n    df_song_info = df_song_info.append(new_row, ignore_index=True)\n\ndf_song_info.to_pickle(\"../../data/other_files/df_song_info.pkl\")\ndf_song_info.head()    ","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":6,"columns":[{"name":"song_id","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"5HQEmiV2lKnSO6qa2fsR7x","count":1},{"name":"1LOZMYF5s8qhW7Rv4w2gun","count":1},{"name":"3 others","count":3}]}},{"name":"artist","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"10cc","count":5}]}},{"name":"artist_id","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"6i6WlGzQtXtz7GcC5H5st5","count":5}]}},{"name":"genre","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"Rock","count":5}]}},{"name":"sentiment","dtype":"float64","stats":{"unique_count":4,"nan_count":0,"min":5.270607902735563,"max":5.7342715231788075,"histogram":[{"bin_start":5.270607902735563,"bin_end":5.316974264779887,"count":1},{"bin_start":5.316974264779887,"bin_end":5.363340626824212,"count":0},{"bin_start":5.363340626824212,"bin_end":5.409706988868536,"count":1},{"bin_start":5.409706988868536,"bin_end":5.4560733509128605,"count":0},{"bin_start":5.4560733509128605,"bin_end":5.502439712957186,"count":0},{"bin_start":5.502439712957186,"bin_end":5.54880607500151,"count":0},{"bin_start":5.54880607500151,"bin_end":5.595172437045834,"count":0},{"bin_start":5.595172437045834,"bin_end":5.641538799090158,"count":0},{"bin_start":5.641538799090158,"bin_end":5.687905161134483,"count":1},{"bin_start":5.687905161134483,"bin_end":5.7342715231788075,"count":2}]}},{"name":"year","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"1975","count":1},{"name":"1978","count":1},{"name":"3 others","count":3}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"song_id":"5HQEmiV2lKnSO6qa2fsR7x","artist":"10cc","artist_id":"6i6WlGzQtXtz7GcC5H5st5","genre":"Rock","sentiment":5.36713004484305,"year":"1975","_deepnote_index_column":0},{"song_id":"1LOZMYF5s8qhW7Rv4w2gun","artist":"10cc","artist_id":"6i6WlGzQtXtz7GcC5H5st5","genre":"Rock","sentiment":5.7342715231788075,"year":"1978","_deepnote_index_column":1},{"song_id":"6KEWtSOGKpIXGw6l1uJgsR","artist":"10cc","artist_id":"6i6WlGzQtXtz7GcC5H5st5","genre":"Rock","sentiment":5.650507246376812,"year":"1977","_deepnote_index_column":2},{"song_id":"1QQgSUKCG8GakzMOwi4lFS","artist":"10cc","artist_id":"6i6WlGzQtXtz7GcC5H5st5","genre":"Rock","sentiment":5.270607902735563,"year":"1973","_deepnote_index_column":3},{"song_id":"4E2gdBRKC12MJWFUOkH0UN","artist":"10cc","artist_id":"6i6WlGzQtXtz7GcC5H5st5","genre":"Rock","sentiment":5.7342715231788075,"year":"2002","_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"                  song_id artist               artist_id genre  sentiment  \\\n0  5HQEmiV2lKnSO6qa2fsR7x   10cc  6i6WlGzQtXtz7GcC5H5st5  Rock   5.367130   \n1  1LOZMYF5s8qhW7Rv4w2gun   10cc  6i6WlGzQtXtz7GcC5H5st5  Rock   5.734272   \n2  6KEWtSOGKpIXGw6l1uJgsR   10cc  6i6WlGzQtXtz7GcC5H5st5  Rock   5.650507   \n3  1QQgSUKCG8GakzMOwi4lFS   10cc  6i6WlGzQtXtz7GcC5H5st5  Rock   5.270608   \n4  4E2gdBRKC12MJWFUOkH0UN   10cc  6i6WlGzQtXtz7GcC5H5st5  Rock   5.734272   \n\n   year  \n0  1975  \n1  1978  \n2  1977  \n3  1973  \n4  2002  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>artist_id</th>\n      <th>genre</th>\n      <th>sentiment</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5HQEmiV2lKnSO6qa2fsR7x</td>\n      <td>10cc</td>\n      <td>6i6WlGzQtXtz7GcC5H5st5</td>\n      <td>Rock</td>\n      <td>5.367130</td>\n      <td>1975</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1LOZMYF5s8qhW7Rv4w2gun</td>\n      <td>10cc</td>\n      <td>6i6WlGzQtXtz7GcC5H5st5</td>\n      <td>Rock</td>\n      <td>5.734272</td>\n      <td>1978</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6KEWtSOGKpIXGw6l1uJgsR</td>\n      <td>10cc</td>\n      <td>6i6WlGzQtXtz7GcC5H5st5</td>\n      <td>Rock</td>\n      <td>5.650507</td>\n      <td>1977</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1QQgSUKCG8GakzMOwi4lFS</td>\n      <td>10cc</td>\n      <td>6i6WlGzQtXtz7GcC5H5st5</td>\n      <td>Rock</td>\n      <td>5.270608</td>\n      <td>1973</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4E2gdBRKC12MJWFUOkH0UN</td>\n      <td>10cc</td>\n      <td>6i6WlGzQtXtz7GcC5H5st5</td>\n      <td>Rock</td>\n      <td>5.734272</td>\n      <td>2002</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Dataframe for artists**  \nTo construct the dataframe with information on artist level we need to assign a year to each artist.\nHowever, as one artist can have songs which are from several different years we will calculate a mean for the release year for all songs and assign this year to the artist. \nIntitially a dicionary containing the average year for each artist id is therefore constructed.","metadata":{"tags":[],"cell_id":"00018-adb3eb38-65d4-4202-9c28-b35950cd2e93","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00018-1a881547-b4ba-410b-bfd3-4cd20f3dbc9d","output_cleared":false,"source_hash":"b5c548ea","execution_start":1607174522104,"execution_millis":0,"deepnote_cell_type":"code"},"source":"# Calulate and save the year of each artist\nartist_year = {}\nfor artist_id in artist_song_id:\n    years = []\n    for song_id in artist_song_id[artist_id]:\n        years.append(int(song_year[song_id][0:4]))\n    artist_year[artist_id] = round(np.mean(years))\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of people the artist has collaborated with is also added to the dataframe.\nThis number will be the same as the edges in the network and thereby also the degree of the artist node.","metadata":{"tags":[],"cell_id":"00021-0ebae152-b86c-402c-bdc6-d13dbfbc94ae","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00021-6e8c238e-a431-4040-9a2f-ffb5e083ba1d","output_cleared":false,"source_hash":"61f2c499","execution_millis":10817,"execution_start":1607175036011,"deepnote_cell_type":"code"},"source":"# get degree of every artist in the network\ndegrees = G.degree()\n\n# construct dataframe for song information\ndf_artist_info = pd.DataFrame(columns=['artist','artist_id','genre','sentiment','year','degree'])\n\nfor artist_id in artist_year.keys():\n    # Get info for new row of dataframe\n    year = artist_year[artist_id]\n    artist = G._node[artist_id]['name']\n    genre = G._node[artist_id]['genre']\n    degree = degrees[artist_id]\n    sentiment = G._node[artist_id]['sentiment']\n    # Specify and append new row\n    new_row = {'artist': artist,'artist_id':artist_id,'genre':genre,\n               'sentiment':sentiment,'year':year,'degree':degree}\n    df_artist_info = df_artist_info.append(new_row, ignore_index=True)\n\ndf_artist_info.to_pickle(\"../../data/other_files/df_artist_info.pkl\")\ndf_artist_info.head() ","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":6,"columns":[{"name":"artist","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"10cc","count":1},{"name":"Paul McCartney","count":1},{"name":"3 others","count":3}]}},{"name":"artist_id","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"6i6WlGzQtXtz7GcC5H5st5","count":1},{"name":"4STHEaNw4mPZ2tzheohgXB","count":1},{"name":"3 others","count":3}]}},{"name":"genre","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"Rock","count":2},{"name":"Pop","count":2},{"name":"Hip-Hop","count":1}]}},{"name":"sentiment","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":5.3713994241898435,"max":5.7118722511266125,"histogram":[{"bin_start":5.3713994241898435,"bin_end":5.40544670688352,"count":2},{"bin_start":5.40544670688352,"bin_end":5.439493989577198,"count":0},{"bin_start":5.439493989577198,"bin_end":5.473541272270874,"count":0},{"bin_start":5.473541272270874,"bin_end":5.507588554964551,"count":0},{"bin_start":5.507588554964551,"bin_end":5.541635837658228,"count":0},{"bin_start":5.541635837658228,"bin_end":5.575683120351905,"count":1},{"bin_start":5.575683120351905,"bin_end":5.609730403045582,"count":1},{"bin_start":5.609730403045582,"bin_end":5.643777685739258,"count":0},{"bin_start":5.643777685739258,"bin_end":5.677824968432936,"count":0},{"bin_start":5.677824968432936,"bin_end":5.7118722511266125,"count":1}]}},{"name":"year","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"1981","count":1},{"name":"1986","count":1},{"name":"3 others","count":3}]}},{"name":"degree","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"2","count":1},{"name":"21","count":1},{"name":"3 others","count":3}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"artist":"10cc","artist_id":"6i6WlGzQtXtz7GcC5H5st5","genre":"Rock","sentiment":5.5513576480626075,"year":1981,"degree":2,"_deepnote_index_column":0},{"artist":"Paul McCartney","artist_id":"4STHEaNw4mPZ2tzheohgXB","genre":"Rock","sentiment":5.7118722511266125,"year":1986,"degree":21,"_deepnote_index_column":1},{"artist":"Rakim","artist_id":"3PyWEKLWI0vHPmoNrIX0QE","genre":"Hip-Hop","sentiment":5.373356721913472,"year":2002,"degree":20,"_deepnote_index_column":2},{"artist":"12th Planet","artist_id":"3V1h3kAdiVDBiwlY2i6dJz","genre":"Pop","sentiment":5.5932550891588395,"year":2017,"degree":6,"_deepnote_index_column":3},{"artist":"Skrillex","artist_id":"5he5w2lnU9x7JFhnwcekXX","genre":"Pop","sentiment":5.3713994241898435,"year":2014,"degree":48,"_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"           artist               artist_id    genre  sentiment  year degree\n0            10cc  6i6WlGzQtXtz7GcC5H5st5     Rock   5.551358  1981      2\n1  Paul McCartney  4STHEaNw4mPZ2tzheohgXB     Rock   5.711872  1986     21\n2           Rakim  3PyWEKLWI0vHPmoNrIX0QE  Hip-Hop   5.373357  2002     20\n3     12th Planet  3V1h3kAdiVDBiwlY2i6dJz      Pop   5.593255  2017      6\n4        Skrillex  5he5w2lnU9x7JFhnwcekXX      Pop   5.371399  2014     48","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>artist_id</th>\n      <th>genre</th>\n      <th>sentiment</th>\n      <th>year</th>\n      <th>degree</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10cc</td>\n      <td>6i6WlGzQtXtz7GcC5H5st5</td>\n      <td>Rock</td>\n      <td>5.551358</td>\n      <td>1981</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Paul McCartney</td>\n      <td>4STHEaNw4mPZ2tzheohgXB</td>\n      <td>Rock</td>\n      <td>5.711872</td>\n      <td>1986</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Rakim</td>\n      <td>3PyWEKLWI0vHPmoNrIX0QE</td>\n      <td>Hip-Hop</td>\n      <td>5.373357</td>\n      <td>2002</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12th Planet</td>\n      <td>3V1h3kAdiVDBiwlY2i6dJz</td>\n      <td>Pop</td>\n      <td>5.593255</td>\n      <td>2017</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Skrillex</td>\n      <td>5he5w2lnU9x7JFhnwcekXX</td>\n      <td>Pop</td>\n      <td>5.371399</td>\n      <td>2014</td>\n      <td>48</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00016-203cb4e6-502c-4500-b580-591a29a9df12","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Wordclouds\n\nThe purpose of this section is to prepare the text so it is ready to be analyzed by wordclouds.\nTo do this a document for each genre is created consisting of all the lyrics within the genre.\n","metadata":{"tags":[],"cell_id":"00012-46d9bf58-3a19-4d29-a136-3e9687d37c8a","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-e22ede47-f99a-416f-825e-dcd0335a29cf","output_cleared":false,"source_hash":"7a54789f","execution_millis":2,"execution_start":1607109722374,"deepnote_cell_type":"code"},"source":"## get the different genres\ngenres = []\nfor nod in G.nodes():\n    genres.append(G._node[nod]['genre'])\ngenres = set(genres)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00014-b11edbc6-07f0-4a42-b7a2-66d17e2b5ba5","output_cleared":false,"source_hash":"a8fc13e8","execution_start":1607109931091,"execution_millis":15704,"deepnote_cell_type":"code"},"source":"# Construct dictionary for the text for each genre\ngenre_text = {}\nfor genre in genres:\n    genre_text[genre] = ''\n\n# Retrieve the text\nfor artist_id in G.nodes():\n    songs = artist_song_id[artist_id] # list of song for the artist\n    song_lyrics = '' # empty string\n    for song in songs:\n        # Load file\n        text_file = open(\"../../data/genius/all_song_lyrics_cleaned/\" + song + \".txt\", \"r\").read()\n        song_lyrics += text_file + ' ' # add all song files\n    genre = G._node[artist_id]['genre'] # Get genre of node\n    # Add the text to the genre in the dictionary\n    genre_text[genre] += song_lyrics + ' '","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function `text_preprocess` is used again to clean the text and the dictionary `genre_text_clean` containing the tokenized words for each genre is saved.","metadata":{"tags":[],"cell_id":"00015-f246f754-1edc-4895-aba0-54df09d213bf","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00015-662c1ce5-9144-4b31-bdca-6259c4877761","output_cleared":false,"source_hash":"8b8aff52","execution_start":1607110058053,"execution_millis":38840,"deepnote_cell_type":"code"},"source":"genre_text_clean = {}\n# Text preprocess the text for each genre\nfor genre in genres:\n    genre_text_clean[genre] = text_preprocess(genre_text[genre])\n\n## Save the dictionary\nwith open(\"../../data/other_files/genre_text_clean.json\", \"w\") as json_file:\n    json.dump(genre_text_clean,json_file)","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"1bd0ba93-2a6d-4aba-a681-007ed5a2bc25","deepnote_execution_queue":[]}}